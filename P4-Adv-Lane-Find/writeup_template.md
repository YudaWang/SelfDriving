# Project4 Advanced Lane Finding
## Yuda Wang 2017/9/10

---

**Advanced Lane Finding Project**

The goals / steps of this project are the following:

* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
* Apply a distortion correction to raw images.
* Use color transforms, gradients, etc., to create a thresholded binary image.
* Apply a perspective transform to rectify binary image ("birds-eye view").
* Detect lane pixels and fit to find the lane boundary.
* Determine the curvature of the lane and vehicle position with respect to center.
* Warp the detected lane boundaries back onto the original image.
* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.

[//]: # (Image References)

[image1]: ./P4-undistort.png "Undistorted"
[image2]: ./P4-colorRGBHLSHSV.png "All color spaces"
[image3]: ./P4-colorgradientchannel.png "Color gradient channels"
[image4]: ./P4-colorgradientResult.png "Color gradient result"
[image5]: ./P4-perspective.png "Perspective transform"
[image6]: ./P4-warped.png "Warped"
[image7]: ./P4-guassianwindow.png "Guassian window used"
[image8]: ./P4-convolvedetectedlane.png "Convolve detected lanes"
[image9]: ./P4-fitting.png "Fitting"
[image10]: ./P4-final.png "Final Result"
[video1]: ./ProjectVideoOutMvAvg.mp4 "ProjectVideo"

## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points

### Camera Calibration

#### 1. Briefly state how you computed the camera matrix and distortion coefficients. 
The camera matrix and distortion coeffs are computed based on a series of chess board images taken by the same camera at different locations w.r.t camera. The regular optical use case of auto-driving camera is to focus object at 1-inf meters. For normal wide angle lens, the difference among the focusing of an object at 1-inf meters is negligible. Hence, the calibration images are only used for (x,y) calibration in the real space. The object points are always the same due to the same 9x6 chess board. The image points are self detected by cv2.findChessboardCorners function. Finally the camera matrix and distorsion coefficients are computed by cv2.calibrateCamera function.

### Pipeline (single images)

#### 1. Provide an example of a distortion-corrected image.

Using the camera matrix and distortion coeffs generated by the prior step, I simply undistorted one example image(straight_lines2.jpg) as below.
![alt text][image1]

#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.

I looked at 3 color spaces, namely RGB/HSV/HLS, to find out the right color channels that could give clear detection of lane line. As shown in the image below, the S channel of HLS and R channel of RGB provided the top two best contrast of the lane lines.
![alt text][image2]
While using grandient intensity or direction selection, it is not clear that lane line detection is always correct. Sometime it won't due to some other line structures in the image(e.g. long cracks on the road). 
![alt text][image3]
So without getting into more complicated algorithm for lane line detection, I decided not to use gradient channel but only color selections(S-channel of HLS / R-channel of RGB) for lane line detection.
![alt text][image4]

#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.

For perspective transformation, I assume the car in the example image straight_lines2.jpg is exactly in the center of the line, which is the best I can tell among all of the images. However, this could be an mistake if the camera is mounted at different locations among different images/videos. To do the transform, I picked the source points as guided by the straight lane lines from straight_lines2.jpg, the destination points are fixed given certain image size.

source points = np.float32([[217,720], [566,470], [720,470], [1113,720]])
destination points = np.float32([[1.0/4*imgShape[1],imgShape[0]],[1.0/4*imgShape[1],1.0/3*imgShape[0]],
                   [3.0/4*imgShape[1],1.0/3*imgShape[0]],[3.0/4*imgShape[1],imgShape[0]]])


![alt text][image5]

The final warped image is given as below.
![alt text][image6]

#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?

Then I did some other stuff and fit my lane lines with a 2nd order polynomial kinda like this:

![alt text][image7]

#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.

I did this in lines # through # in my code in `my_other_file.py`

#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.

I implemented this step in lines # through # in my code in `yet_another_file.py` in the function `map_lane()`.  Here is an example of my result on a test image:

![alt text][image8]

---

### Pipeline (video)

#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).

Here's a [link to my video result](./ProjectVideoOutMvAvg.mp4)

---

### Discussion

#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?

Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.  
