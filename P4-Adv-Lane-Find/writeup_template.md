# Project4 Advanced Lane Finding
## Yuda Wang 2017/9/10

---

**Advanced Lane Finding Project**

The goals / steps of this project are the following:

* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
* Apply a distortion correction to raw images.
* Use color transforms, gradients, etc., to create a thresholded binary image.
* Apply a perspective transform to rectify binary image ("birds-eye view").
* Detect lane pixels and fit to find the lane boundary.
* Determine the curvature of the lane and vehicle position with respect to center.
* Warp the detected lane boundaries back onto the original image.
* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.

[//]: # (Image References)

[image1]: ./P4-undistort.png "Undistorted"
[image2]: ./P4-colorRGBHLSHSV.png "All color spaces"
[image3]: ./P4-colorgradientchannel.png "Color gradient channels"
[image4]: ./P4-colorgradientResult.png "Color gradient result"
[image5]: ./P4-perspective.png "Perspective transform"
[image6]: ./P4-warped.png "Warped"
[image7]: ./P4-guassianwindow.png "Guassian window used"
[image8]: ./P4-convolvedetectedlane.png "Convolve detected lanes"
[image9]: ./P4-fitting.png "Fitting"
[image10]: ./P4-final.png "Final Result"
[video1]: ./ProjectVideoOutMvAvg.mp4 "ProjectVideo"


## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points

### Camera Calibration

#### 1. Briefly state how you computed the camera matrix and distortion coefficients. 
The camera matrix and distortion coeffs are computed based on a series of chess board images taken by the same camera at different locations w.r.t camera. The regular optical use case of auto-driving camera is to focus object at 1-inf meters. For normal wide angle lens, the difference among the focusing of an object at 1-inf meters is negligible. Hence, the calibration images are only used for (x,y) calibration in the real space. The object points are always the same due to the same 9x6 chess board. The image points are self detected by cv2.findChessboardCorners function. Finally the camera matrix and distorsion coefficients are computed by cv2.calibrateCamera function.

### Pipeline (single images)

#### 1. Provide an example of a distortion-corrected image.

Using the camera matrix and distortion coeffs generated by the prior step, I simply undistorted one example image(straight_lines2.jpg) as below.
![alt text][image1]

#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.

I looked at 3 color spaces, namely RGB/HSV/HLS, to find out the right color channels that could give clear detection of lane line. As shown in the image below, the S channel of HLS and R channel of RGB provided the top two best contrast of the lane lines.
![alt text][image2]
While using grandient intensity or direction selection, it is not clear that lane line detection is always correct. Sometime it won't due to some other line structures in the image(e.g. long cracks on the road). 
![alt text][image3]
So without getting into more complicated algorithm for lane line detection, I decided not to use gradient channel but only color selections(S-channel of HLS / R-channel of RGB) for lane line detection.
![alt text][image4]

#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.

For perspective transformation, I assume the car in the example image straight_lines2.jpg is exactly in the center of the line, which is the best I can tell among all of the images. However, this could be an mistake if the camera is mounted at different locations among different images/videos. To do the transform, I picked the source points as guided by the straight lane lines from straight_lines2.jpg, the destination points are fixed given certain image size.

source points = np.float32([[217,720], [566,470], [720,470], [1113,720]])

destination points = np.float32([[1.0/4*imgShape[1],imgShape[0]],[1.0/4*imgShape[1],1.0/3*imgShape[0]],
                   [3.0/4*imgShape[1],1.0/3*imgShape[0]],[3.0/4*imgShape[1],imgShape[0]]])


![alt text][image5]


The final warped image of left/right lanes is given as below.

![alt text][image6]

#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?

Lane line pixels are identified by:
(1) construct a lane line profile by summing over the lower half of the warped image
(2) using a window/margin at the lane line profile maximum positions to for left/right lane line finding
(3) when doing convolution, I used guassian profile moving average horizontal windows to smooth out the current vertical selected section's intensity profile so np.max algorithm can easilty return the max positions, which corresponding to left/right lane lines center.

![alt text][image7]
![alt text][image8]

For Lane fitting, 2nd order polynomial function is used to individually fit left/right lane lines' pixels.
[Note: in final video sequential fitting, instead of using single frame left/right lane lines' pixels, as far as last 10 frame's lane lines' detected pixels will be taken to fit the current curve. Reason to use 10 frame sum is to provide more stable results by taking more sample data points. 10 frames is a reasonable sum since it correspond to <=0.5sec in the real use case (>20 frames/sec), which is a small time regarding vehicle response.]
![alt text][image9]

#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.

First I converted image pixels to meters by using the lane width pixel counts and dashed lane line length pixel counts, which are usually 3.7meters and 3meters in the real word.
Then I converted all lane line data points from pixel positions to meter positions and fit all of them again in meter scale.
Finally some simple math below returns the curvature and offset.


y_max_meter = np.max(ys_meter)

l_curvR_meter = ((1 + (2*l_fit_meter[0]*y_max_meter + l_fit_meter[1])**2)**1.5) / np.absolute(2*l_fit_meter[0])

r_curvR_meter = ((1 + (2*r_fit_meter[0]*y_max_meter + r_fit_meter[1])**2)**1.5) / np.absolute(2*r_fit_meter[0])

vehicle_offset_meter = imgShape[1]/2*x_pixel2meter - (l_interp_meter(y_max_meter)+r_interp_meter(y_max_meter))/2 

#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.

Finally, the detected lane line was outlined and filled with green color and transform back to the original image by perspective transformation.

![alt text][image10]

---

### Pipeline (video)

#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).


![alt text][video1]

Here's a [link to my video result](./ProjectVideoOutMvAvg.mp4)

---


### Discussion

#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?

(1) I did use the prior video frame lane line detection data points for current frame lane detection. However this is not implemented with the lane line class suggested by class. It is ok for now but might cause difficult in debugging in the long term.
(2) The challenge video didn't worked very well. Seem like the lane line detection algo is still very naive, which only worked for ideal case. For more sophisticated road conditions, some more constraints/method shall be introduced to predict/detect lane lines.
